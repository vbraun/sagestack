diff -ru src/mpn/alpha/addlsh1_n.asm src.patched/mpn/alpha/addlsh1_n.asm
--- src/mpn/alpha/addlsh1_n.asm	2010-12-31 08:26:24.000000000 +0100
+++ src.patched/mpn/alpha/addlsh1_n.asm	2012-04-07 14:27:57.058280100 +0200
@@ -54,7 +54,7 @@
 define(`ps', `r25')
 define(`sl', `r28')
 
-define(OPERATION_addlsh1_n,1)
+define(`OPERATION_addlsh1_n',1)
 
 ifdef(`OPERATION_addlsh1_n',`
   define(ADDSUB,       addq)
diff -ru src/mpn/alpha/sublsh1_n.asm src.patched/mpn/alpha/sublsh1_n.asm
--- src/mpn/alpha/sublsh1_n.asm	2010-12-31 08:26:10.000000000 +0100
+++ src.patched/mpn/alpha/sublsh1_n.asm	2012-04-07 14:27:57.048279798 +0200
@@ -54,7 +54,7 @@
 define(`ps', `r25')
 define(`sl', `r28')
 
-define(OPERATION_sublsh1_n,1)
+define(`OPERATION_sublsh1_n',1)
 
 ifdef(`OPERATION_addlsh1_n',`
   define(ADDSUB,       addq)
diff -ru src/mpn/ia64/add_n.asm src.patched/mpn/ia64/add_n.asm
--- src/mpn/ia64/add_n.asm	2010-12-31 08:21:21.000000000 +0100
+++ src.patched/mpn/ia64/add_n.asm	2012-04-07 14:27:57.628279922 +0200
@@ -33,7 +33,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n',`
   define(ADDSUB,	add)
diff -ru src/mpn/ia64/addlsh1_n.asm src.patched/mpn/ia64/addlsh1_n.asm
--- src/mpn/ia64/addlsh1_n.asm	2010-12-31 08:25:53.000000000 +0100
+++ src.patched/mpn/ia64/addlsh1_n.asm	2012-04-07 14:27:57.668283269 +0200
@@ -32,7 +32,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_addlsh1_n,1)
+define(`OPERATION_addlsh1_n',1)
 
 ifdef(`OPERATION_addlsh1_n',`
   define(ADDSUB,       add)
diff -ru src/mpn/ia64/and_n.asm src.patched/mpn/ia64/and_n.asm
--- src/mpn/ia64/and_n.asm	2010-12-31 08:11:37.000000000 +0100
+++ src.patched/mpn/ia64/and_n.asm	2012-04-07 14:27:57.678272121 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_and_n,1)
+define(`OPERATION_and_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/andn_n.asm src.patched/mpn/ia64/andn_n.asm
--- src/mpn/ia64/andn_n.asm	2010-12-31 08:11:10.000000000 +0100
+++ src.patched/mpn/ia64/andn_n.asm	2012-04-07 14:27:57.468269051 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_andn_n,1)
+define(`OPERATION_andn_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/ior_n.asm src.patched/mpn/ia64/ior_n.asm
--- src/mpn/ia64/ior_n.asm	2010-12-31 08:12:01.000000000 +0100
+++ src.patched/mpn/ia64/ior_n.asm	2012-04-07 14:27:57.448269684 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_ior_n,1)
+define(`OPERATION_ior_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/iorn_n.asm src.patched/mpn/ia64/iorn_n.asm
--- src/mpn/ia64/iorn_n.asm	2010-12-31 08:12:11.000000000 +0100
+++ src.patched/mpn/ia64/iorn_n.asm	2012-04-07 14:27:57.658279932 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_iorn_n,1)
+define(`OPERATION_iorn_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/lshift.asm src.patched/mpn/ia64/lshift.asm
--- src/mpn/ia64/lshift.asm	2010-12-31 08:26:45.000000000 +0100
+++ src.patched/mpn/ia64/lshift.asm	2012-04-07 14:27:57.618271813 +0200
@@ -39,7 +39,7 @@
 
 define(`tnc',`r9')
 
-define(OPERATION_lshift,1)
+define(`OPERATION_lshift',1)
 
 ifdef(`OPERATION_lshift',`
 	define(`FSH',`shl')
diff -ru src/mpn/ia64/nand_n.asm src.patched/mpn/ia64/nand_n.asm
--- src/mpn/ia64/nand_n.asm	2010-12-31 08:11:26.000000000 +0100
+++ src.patched/mpn/ia64/nand_n.asm	2012-04-07 14:27:57.488270124 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_nand_n,1)
+define(`OPERATION_nand_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/nior_n.asm src.patched/mpn/ia64/nior_n.asm
--- src/mpn/ia64/nior_n.asm	2010-12-31 08:12:21.000000000 +0100
+++ src.patched/mpn/ia64/nior_n.asm	2012-04-07 14:27:57.548279945 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_nior_n,1)
+define(`OPERATION_nior_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/rsh1add_n.asm src.patched/mpn/ia64/rsh1add_n.asm
--- src/mpn/ia64/rsh1add_n.asm	2010-12-31 08:24:25.000000000 +0100
+++ src.patched/mpn/ia64/rsh1add_n.asm	2012-04-07 14:27:57.538280044 +0200
@@ -33,7 +33,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_rsh1add_n,1)
+define(`OPERATION_rsh1add_n',1)
 
 ifdef(`OPERATION_rsh1add_n',`
   define(ADDSUB,       add)
diff -ru src/mpn/ia64/rsh1sub_n.asm src.patched/mpn/ia64/rsh1sub_n.asm
--- src/mpn/ia64/rsh1sub_n.asm	2010-12-31 08:24:36.000000000 +0100
+++ src.patched/mpn/ia64/rsh1sub_n.asm	2012-04-07 14:27:57.508269882 +0200
@@ -33,7 +33,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_rsh1sub_n,1)
+define(`OPERATION_rsh1sub_n',1)
 
 ifdef(`OPERATION_rsh1add_n',`
   define(ADDSUB,       add)
diff -ru src/mpn/ia64/rshift.asm src.patched/mpn/ia64/rshift.asm
--- src/mpn/ia64/rshift.asm	2010-12-31 08:26:55.000000000 +0100
+++ src.patched/mpn/ia64/rshift.asm	2012-04-07 14:27:57.628279922 +0200
@@ -39,7 +39,7 @@
 
 define(`tnc',`r9')
 
-define(OPERATION_rshift,1)
+define(`OPERATION_rshift',1)
 
 ifdef(`OPERATION_lshift',`
 	define(`FSH',`shl')
diff -ru src/mpn/ia64/sub_n.asm src.patched/mpn/ia64/sub_n.asm
--- src/mpn/ia64/sub_n.asm	2010-12-31 08:21:31.000000000 +0100
+++ src.patched/mpn/ia64/sub_n.asm	2012-04-07 14:27:57.518279963 +0200
@@ -33,7 +33,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n',`
   define(ADDSUB,	add)
diff -ru src/mpn/ia64/sublsh1_n.asm src.patched/mpn/ia64/sublsh1_n.asm
--- src/mpn/ia64/sublsh1_n.asm	2010-12-31 08:25:40.000000000 +0100
+++ src.patched/mpn/ia64/sublsh1_n.asm	2012-04-07 14:27:57.638269756 +0200
@@ -32,7 +32,7 @@
 define(`vp',`r34')
 define(`n',`r35')
 
-define(OPERATION_sublsh1_n,1)
+define(`OPERATION_sublsh1_n',1)
 
 ifdef(`OPERATION_addlsh1_n',`
   define(ADDSUB,       add)
diff -ru src/mpn/ia64/xnor_n.asm src.patched/mpn/ia64/xnor_n.asm
--- src/mpn/ia64/xnor_n.asm	2010-12-31 08:12:45.000000000 +0100
+++ src.patched/mpn/ia64/xnor_n.asm	2012-04-07 14:27:57.608279181 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_xnor_n,1)
+define(`OPERATION_xnor_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/ia64/xor_n.asm src.patched/mpn/ia64/xor_n.asm
--- src/mpn/ia64/xor_n.asm	2010-12-31 08:12:33.000000000 +0100
+++ src.patched/mpn/ia64/xor_n.asm	2012-04-07 14:27:57.608279181 +0200
@@ -34,7 +34,7 @@
 define(`vp', `r34')
 define(`n', `r35')
 
-define(OPERATION_xor_n,1)
+define(`OPERATION_xor_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/and_n.asm src.patched/mpn/powerpc32/vmx/and_n.asm
--- src/mpn/powerpc32/vmx/and_n.asm	2010-12-31 08:16:01.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/and_n.asm	2012-04-07 14:27:52.968280448 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_and_n,1)
+define(`OPERATION_and_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/andn_n.asm src.patched/mpn/powerpc32/vmx/andn_n.asm
--- src/mpn/powerpc32/vmx/andn_n.asm	2010-12-31 08:16:13.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/andn_n.asm	2012-04-07 14:27:52.918280512 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_andn_n,1)
+define(`OPERATION_andn_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/ior_n.asm src.patched/mpn/powerpc32/vmx/ior_n.asm
--- src/mpn/powerpc32/vmx/ior_n.asm	2010-12-31 08:16:39.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/ior_n.asm	2012-04-07 14:27:52.918280512 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_ior_n,1)
+define(`OPERATION_ior_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/iorn_n.asm src.patched/mpn/powerpc32/vmx/iorn_n.asm
--- src/mpn/powerpc32/vmx/iorn_n.asm	2010-12-31 08:16:51.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/iorn_n.asm	2012-04-07 14:27:52.968280448 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_iorn_n,1)
+define(`OPERATION_iorn_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/nand_n.asm src.patched/mpn/powerpc32/vmx/nand_n.asm
--- src/mpn/powerpc32/vmx/nand_n.asm	2010-12-31 08:16:26.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/nand_n.asm	2012-04-07 14:27:52.928276559 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_nand_n,1)
+define(`OPERATION_nand_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/nior_n.asm src.patched/mpn/powerpc32/vmx/nior_n.asm
--- src/mpn/powerpc32/vmx/nior_n.asm	2010-12-31 08:17:04.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/nior_n.asm	2012-04-07 14:27:52.938268875 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_nior_n,1)
+define(`OPERATION_nior_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/xnor_n.asm src.patched/mpn/powerpc32/vmx/xnor_n.asm
--- src/mpn/powerpc32/vmx/xnor_n.asm	2010-12-31 08:17:31.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/xnor_n.asm	2012-04-07 14:27:52.958279999 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_xnor_n,1)
+define(`OPERATION_xnor_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/powerpc32/vmx/xor_n.asm src.patched/mpn/powerpc32/vmx/xor_n.asm
--- src/mpn/powerpc32/vmx/xor_n.asm	2010-12-31 08:17:18.000000000 +0100
+++ src.patched/mpn/powerpc32/vmx/xor_n.asm	2012-04-07 14:27:52.958279999 +0200
@@ -56,7 +56,7 @@
 define(`vnegb', `')		C default neg-before to null
 define(`vnega', `')		C default neg-before to null
 
-define(OPERATION_xor_n,1)
+define(`OPERATION_xor_n',1)
 
 ifdef(`OPERATION_and_n',
 `	define(`func',	`mpn_and_n')
diff -ru src/mpn/x86/add_n.asm src.patched/mpn/x86/add_n.asm
--- src/mpn/x86/add_n.asm	2010-12-31 08:30:17.000000000 +0100
+++ src.patched/mpn/x86/add_n.asm	2012-04-07 14:27:56.118269524 +0200
@@ -30,7 +30,7 @@
 C K7:   2.25
 C P4:   8.75
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n',`
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/addmul_1.asm src.patched/mpn/x86/addmul_1.asm
--- src/mpn/x86/addmul_1.asm	2010-12-31 08:21:58.000000000 +0100
+++ src.patched/mpn/x86/addmul_1.asm	2012-04-07 14:27:55.848269439 +0200
@@ -38,7 +38,7 @@
 C K7:                            5.25
 C K8:
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
       define(M4_inst,        addl)
diff -ru src/mpn/x86/applenopic/addmul_1.asm src.patched/mpn/x86/applenopic/addmul_1.asm
--- src/mpn/x86/applenopic/addmul_1.asm	2010-12-31 08:22:55.000000000 +0100
+++ src.patched/mpn/x86/applenopic/addmul_1.asm	2012-04-07 14:27:56.058280957 +0200
@@ -38,7 +38,7 @@
 C K7:                            5.25
 C K8:
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
       define(M4_inst,        addl)
diff -ru src/mpn/x86/applenopic/submul_1.asm src.patched/mpn/x86/applenopic/submul_1.asm
--- src/mpn/x86/applenopic/submul_1.asm	2010-12-31 08:23:06.000000000 +0100
+++ src.patched/mpn/x86/applenopic/submul_1.asm	2012-04-07 14:27:56.028279791 +0200
@@ -38,7 +38,7 @@
 C K7:                            5.25
 C K8:
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
       define(M4_inst,        addl)
diff -ru src/mpn/x86/core2/add_n.asm src.patched/mpn/x86/core2/add_n.asm
--- src/mpn/x86/core2/add_n.asm	2010-12-31 08:30:57.000000000 +0100
+++ src.patched/mpn/x86/core2/add_n.asm	2012-04-07 14:27:55.428291699 +0200
@@ -35,7 +35,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/core2/sub_n.asm src.patched/mpn/x86/core2/sub_n.asm
--- src/mpn/x86/core2/sub_n.asm	2010-12-31 08:31:08.000000000 +0100
+++ src.patched/mpn/x86/core2/sub_n.asm	2012-04-07 14:27:55.368279962 +0200
@@ -35,7 +35,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/k6/add_n.asm src.patched/mpn/x86/k6/add_n.asm
--- src/mpn/x86/k6/add_n.asm	2010-12-31 08:29:14.000000000 +0100
+++ src.patched/mpn/x86/k6/add_n.asm	2012-04-07 14:27:55.778269241 +0200
@@ -24,7 +24,7 @@
 
 C K6: normal 3.25 cycles/limb, in-place 2.75 cycles/limb.
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/k6/addmul_1.asm src.patched/mpn/x86/k6/addmul_1.asm
--- src/mpn/x86/k6/addmul_1.asm	2010-12-31 08:29:47.000000000 +0100
+++ src.patched/mpn/x86/k6/addmul_1.asm	2012-04-07 14:27:55.628279965 +0200
@@ -52,7 +52,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/k6/mmx/and_n.asm src.patched/mpn/x86/k6/mmx/and_n.asm
--- src/mpn/x86/k6/mmx/and_n.asm	2010-12-31 08:13:39.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/and_n.asm	2012-04-07 14:27:55.758280292 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_and_n,1)
+define(`OPERATION_and_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/andn_n.asm src.patched/mpn/x86/k6/mmx/andn_n.asm
--- src/mpn/x86/k6/mmx/andn_n.asm	2010-12-31 08:13:53.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/andn_n.asm	2012-04-07 14:27:55.668280102 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_andn_n,1)
+define(`OPERATION_andn_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/hamdist.asm src.patched/mpn/x86/k6/mmx/hamdist.asm
--- src/mpn/x86/k6/mmx/hamdist.asm	2010-12-31 08:19:05.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/hamdist.asm	2012-04-07 14:27:55.698278894 +0200
@@ -34,7 +34,7 @@
 C The code here isn't optimal, but it's already a 2x speedup over the plain
 C integer mpn/generic/popcount.c,hamdist.c.
 
-define(OPERATION_hamdist,1)
+define(`OPERATION_hamdist',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/k6/mmx/ior_n.asm src.patched/mpn/x86/k6/mmx/ior_n.asm
--- src/mpn/x86/k6/mmx/ior_n.asm	2010-12-31 08:14:20.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/ior_n.asm	2012-04-07 14:27:55.668280102 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_ior_n,1)
+define(`OPERATION_ior_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/iorn_n.asm src.patched/mpn/x86/k6/mmx/iorn_n.asm
--- src/mpn/x86/k6/mmx/iorn_n.asm	2010-12-31 08:14:32.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/iorn_n.asm	2012-04-07 14:27:55.748280101 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_iorn_n,1)
+define(`OPERATION_iorn_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/nand_n.asm src.patched/mpn/x86/k6/mmx/nand_n.asm
--- src/mpn/x86/k6/mmx/nand_n.asm	2010-12-31 08:14:07.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/nand_n.asm	2012-04-07 14:27:55.678270024 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_nand_n,1)
+define(`OPERATION_nand_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/nior_n.asm src.patched/mpn/x86/k6/mmx/nior_n.asm
--- src/mpn/x86/k6/mmx/nior_n.asm	2010-12-31 08:14:43.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/nior_n.asm	2012-04-07 14:27:55.708275203 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_nior_n,1)
+define(`OPERATION_nior_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/popcount.asm src.patched/mpn/x86/k6/mmx/popcount.asm
--- src/mpn/x86/k6/mmx/popcount.asm	2010-12-31 08:18:54.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/popcount.asm	2012-04-07 14:27:55.688279899 +0200
@@ -34,7 +34,7 @@
 C The code here isn't optimal, but it's already a 2x speedup over the plain
 C integer mpn/generic/popcount.c,hamdist.c.
 
-define(OPERATION_popcount,1)
+define(`OPERATION_popcount',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/k6/mmx/xnor_n.asm src.patched/mpn/x86/k6/mmx/xnor_n.asm
--- src/mpn/x86/k6/mmx/xnor_n.asm	2010-12-31 08:15:05.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/xnor_n.asm	2012-04-07 14:27:55.718279824 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_xnor_n,1)
+define(`OPERATION_xnor_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/mmx/xor_n.asm src.patched/mpn/x86/k6/mmx/xor_n.asm
--- src/mpn/x86/k6/mmx/xor_n.asm	2010-12-31 08:14:55.000000000 +0100
+++ src.patched/mpn/x86/k6/mmx/xor_n.asm	2012-04-07 14:27:55.728269986 +0200
@@ -41,7 +41,7 @@
 dnl  M4_*_neg_dst means whether to negate the final result before writing
 dnl  M4_*_neg_src2 means whether to negate the src2 values before using them
 
-define(OPERATION_xor_n,1)
+define(`OPERATION_xor_n',1)
 
 define(M4_choose_op,
 m4_assert_numargs(7)
diff -ru src/mpn/x86/k6/sub_n.asm src.patched/mpn/x86/k6/sub_n.asm
--- src/mpn/x86/k6/sub_n.asm	2010-12-31 08:29:24.000000000 +0100
+++ src.patched/mpn/x86/k6/sub_n.asm	2012-04-07 14:27:55.598269852 +0200
@@ -24,7 +24,7 @@
 
 C K6: normal 3.25 cycles/limb, in-place 2.75 cycles/limb.
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/k6/submul_1.asm src.patched/mpn/x86/k6/submul_1.asm
--- src/mpn/x86/k6/submul_1.asm	2010-12-31 08:29:58.000000000 +0100
+++ src.patched/mpn/x86/k6/submul_1.asm	2012-04-07 14:27:55.608280112 +0200
@@ -52,7 +52,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/k7/add_n.asm src.patched/mpn/x86/k7/add_n.asm
--- src/mpn/x86/k7/add_n.asm	2010-12-31 08:27:25.000000000 +0100
+++ src.patched/mpn/x86/k7/add_n.asm	2012-04-07 14:27:56.808269459 +0200
@@ -35,7 +35,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/k7/addmul_1.asm src.patched/mpn/x86/k7/addmul_1.asm
--- src/mpn/x86/k7/addmul_1.asm	2010-12-31 08:28:08.000000000 +0100
+++ src.patched/mpn/x86/k7/addmul_1.asm	2012-04-07 14:27:56.698279971 +0200
@@ -47,7 +47,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/k7/mmx/hamdist.asm src.patched/mpn/x86/k7/mmx/hamdist.asm
--- src/mpn/x86/k7/mmx/hamdist.asm	2010-12-31 08:20:40.000000000 +0100
+++ src.patched/mpn/x86/k7/mmx/hamdist.asm	2012-04-07 14:27:56.728280013 +0200
@@ -38,7 +38,7 @@
 C MMX instructions" in the Athlon Optimization Guide, AMD document 22007,
 C page 158 of rev E (reference in mpn/x86/k7/README).
 
-define(OPERATION_hamdist,1)
+define(`OPERATION_hamdist',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/k7/mmx/popcount.asm src.patched/mpn/x86/k7/mmx/popcount.asm
--- src/mpn/x86/k7/mmx/popcount.asm	2010-12-31 08:20:30.000000000 +0100
+++ src.patched/mpn/x86/k7/mmx/popcount.asm	2012-04-07 14:27:56.728280013 +0200
@@ -38,7 +38,7 @@
 C MMX instructions" in the Athlon Optimization Guide, AMD document 22007,
 C page 158 of rev E (reference in mpn/x86/k7/README).
 
-define(OPERATION_popcount,1)
+define(`OPERATION_popcount',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/k7/sub_n.asm src.patched/mpn/x86/k7/sub_n.asm
--- src/mpn/x86/k7/sub_n.asm	2010-12-31 08:27:37.000000000 +0100
+++ src.patched/mpn/x86/k7/sub_n.asm	2012-04-07 14:27:56.668269259 +0200
@@ -35,7 +35,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n', `
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/k7/submul_1.asm src.patched/mpn/x86/k7/submul_1.asm
--- src/mpn/x86/k7/submul_1.asm	2010-12-31 08:28:18.000000000 +0100
+++ src.patched/mpn/x86/k7/submul_1.asm	2012-04-07 14:27:56.678279889 +0200
@@ -47,7 +47,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/nehalem/hamdist.asm src.patched/mpn/x86/nehalem/hamdist.asm
--- src/mpn/x86/nehalem/hamdist.asm	2010-12-31 08:20:07.000000000 +0100
+++ src.patched/mpn/x86/nehalem/hamdist.asm	2012-04-07 14:27:55.478269000 +0200
@@ -38,7 +38,7 @@
 C might be possible, but 8.5 c/l relying on out-of-order execution is
 C already quite reasonable.
 
-define(OPERATION_hamdist,1)
+define(`OPERATION_hamdist',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/nehalem/popcount.asm src.patched/mpn/x86/nehalem/popcount.asm
--- src/mpn/x86/nehalem/popcount.asm	2010-12-31 08:19:57.000000000 +0100
+++ src.patched/mpn/x86/nehalem/popcount.asm	2012-04-07 14:27:55.468288235 +0200
@@ -38,7 +38,7 @@
 C might be possible, but 8.5 c/l relying on out-of-order execution is
 C already quite reasonable.
 
-define(OPERATION_popcount,1)
+define(`OPERATION_popcount',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/p6/addmul_1.asm src.patched/mpn/x86/p6/addmul_1.asm
--- src/mpn/x86/p6/addmul_1.asm	2012-10-03 22:07:16.000000000 +0200
+++ src.patched/mpn/x86/p6/addmul_1.asm	2012-11-29 13:27:59.761725414 +0100
@@ -46,7 +46,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/p6/submul_1.asm src.patched/mpn/x86/p6/submul_1.asm
--- src/mpn/x86/p6/submul_1.asm	2012-10-03 22:07:16.000000000 +0200
+++ src.patched/mpn/x86/p6/submul_1.asm	2012-11-29 13:27:59.681725417 +0100
@@ -46,7 +46,7 @@
 
 deflit(UNROLL_COUNT, 16)
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
 	define(M4_inst,        addl)
diff -ru src/mpn/x86/pentium/add_n.asm src.patched/mpn/x86/pentium/add_n.asm
--- src/mpn/x86/pentium/add_n.asm	2010-12-31 08:32:53.000000000 +0100
+++ src.patched/mpn/x86/pentium/add_n.asm	2012-04-07 14:27:56.318273599 +0200
@@ -25,7 +25,7 @@
 
 C P5: 2.375 cycles/limb
 
-define(OPERATION_add_n,1)
+define(`OPERATION_add_n',1)
 
 ifdef(`OPERATION_add_n',`
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/pentium/addmul_1.asm src.patched/mpn/x86/pentium/addmul_1.asm
--- src/mpn/x86/pentium/addmul_1.asm	2010-12-31 08:33:16.000000000 +0100
+++ src.patched/mpn/x86/pentium/addmul_1.asm	2012-04-07 14:27:56.238269339 +0200
@@ -25,7 +25,7 @@
 
 C P5: 14.0 cycles/limb
 
-define(OPERATION_addmul_1,1)
+define(`OPERATION_addmul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
       define(M4_inst,        addl)
diff -ru src/mpn/x86/pentium/and_n.asm src.patched/mpn/x86/pentium/and_n.asm
--- src/mpn/x86/pentium/and_n.asm	2010-12-31 07:06:04.000000000 +0100
+++ src.patched/mpn/x86/pentium/and_n.asm	2012-04-07 14:27:56.348273028 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_and_n,1)
+define(`OPERATION_and_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/andn_n.asm src.patched/mpn/x86/pentium/andn_n.asm
--- src/mpn/x86/pentium/andn_n.asm	2010-12-31 07:08:32.000000000 +0100
+++ src.patched/mpn/x86/pentium/andn_n.asm	2012-04-07 14:27:56.148279866 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_andn_n,1)
+define(`OPERATION_andn_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/ior_n.asm src.patched/mpn/x86/pentium/ior_n.asm
--- src/mpn/x86/pentium/ior_n.asm	2010-12-31 07:00:34.000000000 +0100
+++ src.patched/mpn/x86/pentium/ior_n.asm	2012-04-07 14:27:56.128280857 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_ior_n,1)
+define(`OPERATION_ior_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/iorn_n.asm src.patched/mpn/x86/pentium/iorn_n.asm
--- src/mpn/x86/pentium/iorn_n.asm	2010-12-31 07:01:04.000000000 +0100
+++ src.patched/mpn/x86/pentium/iorn_n.asm	2012-04-07 14:27:56.338280957 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_iorn_n,1)
+define(`OPERATION_iorn_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/nand_n.asm src.patched/mpn/x86/pentium/nand_n.asm
--- src/mpn/x86/pentium/nand_n.asm	2010-12-31 07:08:55.000000000 +0100
+++ src.patched/mpn/x86/pentium/nand_n.asm	2012-04-07 14:27:56.158270225 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_nand_n,1)
+define(`OPERATION_nand_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/nior_n.asm src.patched/mpn/x86/pentium/nior_n.asm
--- src/mpn/x86/pentium/nior_n.asm	2010-12-31 07:01:31.000000000 +0100
+++ src.patched/mpn/x86/pentium/nior_n.asm	2012-04-07 14:27:56.188269997 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_nior_n,1)
+define(`OPERATION_nior_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/sub_n.asm src.patched/mpn/x86/pentium/sub_n.asm
--- src/mpn/x86/pentium/sub_n.asm	2010-12-31 08:33:04.000000000 +0100
+++ src.patched/mpn/x86/pentium/sub_n.asm	2012-04-07 14:27:56.178269643 +0200
@@ -25,7 +25,7 @@
 
 C P5: 2.375 cycles/limb
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n',`
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/pentium/submul_1.asm src.patched/mpn/x86/pentium/submul_1.asm
--- src/mpn/x86/pentium/submul_1.asm	2010-12-31 08:33:30.000000000 +0100
+++ src.patched/mpn/x86/pentium/submul_1.asm	2012-04-07 14:27:56.198268327 +0200
@@ -25,7 +25,7 @@
 
 C P5: 14.0 cycles/limb
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1', `
       define(M4_inst,        addl)
diff -ru src/mpn/x86/pentium/xnor_n.asm src.patched/mpn/x86/pentium/xnor_n.asm
--- src/mpn/x86/pentium/xnor_n.asm	2010-12-31 07:09:38.000000000 +0100
+++ src.patched/mpn/x86/pentium/xnor_n.asm	2012-04-07 14:27:56.258279761 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_xnor_n,1)
+define(`OPERATION_xnor_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium/xor_n.asm src.patched/mpn/x86/pentium/xor_n.asm
--- src/mpn/x86/pentium/xor_n.asm	2010-12-31 07:09:16.000000000 +0100
+++ src.patched/mpn/x86/pentium/xor_n.asm	2012-04-07 14:27:56.268279961 +0200
@@ -25,7 +25,7 @@
 C P5: 3.0 c/l  and, ior, xor
 C     3.5 c/l  andn, iorn, nand, nior, xnor
 
-define(OPERATION_xor_n,1)
+define(`OPERATION_xor_n',1)
 
 define(M4_choose_op,
 `ifdef(`OPERATION_$1',`
diff -ru src/mpn/x86/pentium4/mmx/hamdist.asm src.patched/mpn/x86/pentium4/mmx/hamdist.asm
--- src/mpn/x86/pentium4/mmx/hamdist.asm	2010-12-31 08:19:38.000000000 +0100
+++ src.patched/mpn/x86/pentium4/mmx/hamdist.asm	2012-04-07 14:27:56.508279960 +0200
@@ -38,7 +38,7 @@
 C might be possible, but 8.5 c/l relying on out-of-order execution is
 C already quite reasonable.
 
-define(OPERATION_hamdist,1)
+define(`OPERATION_hamdist',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/pentium4/mmx/popcount.asm src.patched/mpn/x86/pentium4/mmx/popcount.asm
--- src/mpn/x86/pentium4/mmx/popcount.asm	2010-12-31 08:19:29.000000000 +0100
+++ src.patched/mpn/x86/pentium4/mmx/popcount.asm	2012-04-07 14:27:56.498279890 +0200
@@ -38,7 +38,7 @@
 C might be possible, but 8.5 c/l relying on out-of-order execution is
 C already quite reasonable.
 
-define(OPERATION_popcount,1)
+define(`OPERATION_popcount',1)
 
 ifdef(`OPERATION_popcount',,
 `ifdef(`OPERATION_hamdist',,
diff -ru src/mpn/x86/sub_n.asm src.patched/mpn/x86/sub_n.asm
--- src/mpn/x86/sub_n.asm	2010-12-31 08:30:28.000000000 +0100
+++ src.patched/mpn/x86/sub_n.asm	2012-04-07 14:27:55.438279940 +0200
@@ -30,7 +30,7 @@
 C K7:   2.25
 C P4:   8.75
 
-define(OPERATION_sub_n,1)
+define(`OPERATION_sub_n',1)
 
 ifdef(`OPERATION_add_n',`
 	define(M4_inst,        adcl)
diff -ru src/mpn/x86/submul_1.asm src.patched/mpn/x86/submul_1.asm
--- src/mpn/x86/submul_1.asm	2010-12-31 08:21:49.000000000 +0100
+++ src.patched/mpn/x86/submul_1.asm	2012-04-07 14:27:55.808285202 +0200
@@ -38,7 +38,7 @@
 C K7:                            5.25
 C K8:
 
-define(OPERATION_submul_1,1)
+define(`OPERATION_submul_1',1)
 
 ifdef(`OPERATION_addmul_1',`
       define(M4_inst,        addl)
